{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from fuzzywuzzy import fuzz\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Dataset/fact_checks.json', 'r') as f:\n",
    "    fact_check_db = json.load(f)\n",
    "\n",
    "num_facts_to_load = int(len(fact_check_db) * 0.1)\n",
    "facts_subset = fact_check_db[:num_facts_to_load]\n",
    "print(len(facts_subset))\n",
    "\n",
    "with open('./Dataset/posts.json', 'r') as f:\n",
    "    posts = json.load(f)\n",
    "num_posts_to_load = int(len(posts) * 0.3)\n",
    "posts_subset = posts[:num_posts_to_load]\n",
    "print(len(posts_subset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model** Implementation-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(multi=True):\n",
    "#     if multi:\n",
    "#         model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "#     else:\n",
    "#         model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     return model\n",
    "\n",
    "# def semantic_clustering(corpus, model, k=10):\n",
    "#     corpus_embeddings = model.encode(corpus)\n",
    "    \n",
    "#     kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "#     clusters = kmeans.fit_predict(corpus_embeddings)\n",
    "\n",
    "#     cluster_groups = {i: [] for i in range(k)}\n",
    "#     for idx, cluster_id in enumerate(clusters):\n",
    "#         cluster_groups[cluster_id].append(corpus[idx])\n",
    "    \n",
    "#     labels = assign_labels(cluster_groups)\n",
    "    \n",
    "#     return clusters, labels\n",
    "\n",
    "# def assign_labels(cluster_groups):\n",
    "#     generator = pipeline('text-generation', model='gpt2')\n",
    "#     labels = []\n",
    "#     for cluster_id, items in cluster_groups.items():\n",
    "#         prompt = f\"Assign a semantic label to the following claims:\\n\"\n",
    "#         for item in items[:5]:  # Limit to first 5 items to avoid exceeding max length\n",
    "#             prompt += f\"- {item}\\n\"\n",
    "#         prompt += \"Label:\"\n",
    "        \n",
    "#         response = generator(prompt, max_length=len(prompt) + 10, num_return_sequences=1)\n",
    "#         label = response[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "#         labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# def fuzzymatch(query, labels, cluster_groups):\n",
    "#     matched_clusters = []\n",
    "#     for idx, label in enumerate(labels):\n",
    "#         score = fuzz.ratio(query, label)\n",
    "#         if score > 60:  # Adjust threshold\n",
    "#             matched_clusters.append(cluster_groups[idx])\n",
    "#     return matched_clusters\n",
    "\n",
    "# def evaluate_supportiveness(query, cluster):\n",
    "#     classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "    \n",
    "#     supportive_claims = []\n",
    "#     for claim in cluster:\n",
    "#         text = f\"Query: {query}\\nClaim: {claim}\"\n",
    "#         result = classifier(text)[0]\n",
    "#         if result['label'] == 'POSITIVE' and result['score'] > 0.6:  # Adjust threshold as needed\n",
    "#             supportive_claims.append(claim)\n",
    "    \n",
    "#     return supportive_claims\n",
    "\n",
    "# # Main function\n",
    "# def fasttrack_algorithm(queries, corpus, multi=True):\n",
    "#     model = load_model(multi)\n",
    "#     Dsel = []\n",
    "    \n",
    "#     # Stage 1: Semantic Clustering\n",
    "#     clusters, labels = semantic_clustering(corpus, model, k=10)\n",
    "    \n",
    "#     # Stage 2: Tracing (for each query)\n",
    "#     for query in queries:\n",
    "#         Dq = [] \n",
    "        \n",
    "#         matched_clusters = fuzzymatch(query, labels, clusters)\n",
    "        \n",
    "#         for cluster in matched_clusters:\n",
    "#             supportive_claims = evaluate_supportiveness(query, cluster)\n",
    "#             Dq.extend(supportive_claims)\n",
    "        \n",
    "#         Dsel.extend(Dq)\n",
    "    \n",
    "#     return Dsel\n",
    "\n",
    "\n",
    "# queries = [\n",
    "#     f\"{post['translated_ocr']}\"\n",
    "#     for post in posts_subset\n",
    "# ]\n",
    "\n",
    "# corpus = [\n",
    "#     f\"{fact['translation_sentence']}\"\n",
    "#     for fact in facts_subset\n",
    "# ]\n",
    "# fasttrack_algorithm(queries,corpus,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model** Implementation-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device):\n",
    "    model = SentenceTransformer('xlm-roberta-base')\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def semantic_clustering(corpus, model, k=10, device='cuda'):\n",
    "    corpus_embeddings = model.encode(corpus, device=device)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    clusters = kmeans.fit_predict(corpus_embeddings.cpu().numpy())\n",
    "\n",
    "    cluster_groups = {i: [] for i in range(k)}\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        cluster_groups[cluster_id].append(corpus[idx])\n",
    "    \n",
    "    labels = assign_labels(cluster_groups, device)\n",
    "    \n",
    "    return clusters, labels\n",
    "\n",
    "def assign_labels(cluster_groups, device):\n",
    "    generator = pipeline('text-generation', model='gpt2', device=0 if device == 'cuda' else -1)\n",
    "    labels = []\n",
    "    for cluster_id, items in cluster_groups.items():\n",
    "        prompt = f\"Assign a semantic label to the following claims:\\n\"\n",
    "        for item in items[:5]:  # Limit to first 5 items to avoid exceeding max length\n",
    "            prompt += f\"- {item}\\n\"\n",
    "        prompt += \"Label:\"\n",
    "        \n",
    "        response = generator(prompt, max_length=len(prompt) + 10, num_return_sequences=1)\n",
    "        label = response[0]['generated_text'].split(\"Label:\")[-1].strip()\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def cross_lingual_similarity(query, claim, model, device):\n",
    "    query_embedding = model.encode(query, device=device)\n",
    "    claim_embedding = model.encode(claim, device=device)\n",
    "    return torch.cosine_similarity(query_embedding, claim_embedding, dim=0).item()\n",
    "\n",
    "def evaluate_relevance(query, claim, tokenizer, model, device):\n",
    "    inputs = tokenizer(query, claim, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    relevance_score = torch.softmax(outputs.logits, dim=1)[0][1].item()  # Assuming 1 is the relevant class\n",
    "    return relevance_score\n",
    "\n",
    "def fasttrack_algorithm(queries, corpus, device='cuda'):\n",
    "    if not torch.cuda.is_available() and device == 'cuda':\n",
    "        print(\"CUDA is not available. Using CPU instead.\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    model = load_model(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-fact-check-relevance-model\")\n",
    "    relevance_model = AutoModelForSequenceClassification.from_pretrained(\"fine-tuned-fact-check-relevance-model\").to(device)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Stage 1: Semantic Clustering\n",
    "    clusters, labels = semantic_clustering(corpus, model, k=10, device=device)\n",
    "    \n",
    "    # Stage 2: Retrieval and Ranking\n",
    "    for query in queries:\n",
    "        relevant_claims = []\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            for claim in cluster:\n",
    "                similarity = cross_lingual_similarity(query, claim, model, device)\n",
    "                relevance = evaluate_relevance(query, claim, tokenizer, relevance_model, device)\n",
    "                \n",
    "                if similarity > 0.5 and relevance > 0.5:  # Adjust thresholds as needed\n",
    "                    relevant_claims.append((claim, similarity * relevance))\n",
    "        \n",
    "        # Rank the relevant claims\n",
    "        ranked_claims = sorted(relevant_claims, key=lambda x: x[1], reverse=True)\n",
    "        results.append(ranked_claims[:10])  # Top 10 most relevant claims\n",
    "    \n",
    "    return results\n",
    "\n",
    "queries = [\n",
    "    f\"{post['translated_ocr']}\"\n",
    "    for post in posts_subset\n",
    "]\n",
    "\n",
    "corpus = [\n",
    "    f\"{fact['translation_sentence']}\"\n",
    "    for fact in facts_subset\n",
    "]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "fasttrack_algorithm(queries, corpus, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
